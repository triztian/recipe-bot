#!/usr/bin/env python3
"""
Retrieval Evaluation for HW4 RAG System

Evaluates BM25 retrieval performance using synthetic queries and calculates
standard information retrieval metrics: Recall@1, Recall@3, Recall@5, Recall@10, and MRR.
"""

import json
import sys
from pathlib import Path
from typing import List, Dict, Any
import statistics

# Add backend to path for imports
sys.path.append(str(Path(__file__).parent.parent.parent.parent / "backend"))
from retrieval import create_retriever
from evaluation_utils import BaseRetrievalEvaluator, load_queries


class RetrievalEvaluator(BaseRetrievalEvaluator):
    """Extended evaluator with additional analysis for HW4."""
    
    def analyze_by_query_characteristics(self, results: List[Dict[str, Any]]):
        """Analyze performance by query characteristics."""
        print(f"\n--- Additional Analysis ---")
        
        # Analyze by query length
        short_queries = [r for r in results if len(r['original_query'].split()) <= 8]
        long_queries = [r for r in results if len(r['original_query'].split()) > 8]
        
        if short_queries and long_queries:
            short_recall = statistics.mean([r['recall_5'] for r in short_queries])
            long_recall = statistics.mean([r['recall_5'] for r in long_queries])
            print(f"Short queries (â‰¤8 words) Recall@5: {short_recall:.3f} ({len(short_queries)} queries)")
            print(f"Long queries (>8 words) Recall@5: {long_recall:.3f} ({len(long_queries)} queries)")
        
        # Analyze by recipe complexity
        complex_recipes = [r for r in results if len(r['salient_fact'].split()) > 10]
        simple_recipes = [r for r in results if len(r['salient_fact'].split()) <= 10]
        
        if complex_recipes and simple_recipes:
            complex_recall = statistics.mean([r['recall_5'] for r in complex_recipes])
            simple_recall = statistics.mean([r['recall_5'] for r in simple_recipes])
            print(f"Complex salient facts Recall@5: {complex_recall:.3f} ({len(complex_recipes)} queries)")
            print(f"Simple salient facts Recall@5: {simple_recall:.3f} ({len(simple_recipes)} queries)")
    
    def print_final_summary(self, results: List[Dict[str, Any]]):
        """Print comprehensive final summary."""
        final_metrics = self.calculate_aggregate_metrics(results)
        print(f"\n{'='*80}")
        print("FINAL EVALUATION SUMMARY")
        print(f"{'='*80}")
        print(f"ðŸ“Š Overall Performance:")
        print(f"   â€¢ Recall@1:  {final_metrics['recall_at_1']:.3f} ({final_metrics['recall_at_1']*100:.1f}%)")
        print(f"   â€¢ Recall@3:  {final_metrics['recall_at_3']:.3f} ({final_metrics['recall_at_3']*100:.1f}%)")
        print(f"   â€¢ Recall@5:  {final_metrics['recall_at_5']:.3f} ({final_metrics['recall_at_5']*100:.1f}%)")
        print(f"   â€¢ Recall@10: {final_metrics['recall_at_10']:.3f} ({final_metrics['recall_at_10']*100:.1f}%)")
        print(f"   â€¢ MRR:       {final_metrics['mean_reciprocal_rank']:.3f}")
        
        print(f"\nðŸ“ˆ Query Success:")
        print(f"   â€¢ Total queries evaluated: {final_metrics['total_queries']}")
        print(f"   â€¢ Target found (any rank): {final_metrics['queries_found']} ({final_metrics['queries_found']/final_metrics['total_queries']*100:.1f}%)")
        print(f"   â€¢ Target not found:        {final_metrics['queries_not_found']} ({final_metrics['queries_not_found']/final_metrics['total_queries']*100:.1f}%)")
        
        if final_metrics['average_rank_when_found']:
            print(f"\nðŸŽ¯ Ranking Analysis:")
            print(f"   â€¢ Average rank when found: {final_metrics['average_rank_when_found']:.2f}")
            print(f"   â€¢ Median rank when found:  {final_metrics['median_rank_when_found']:.0f}")
        
        print(f"\nðŸ’¡ Performance Insights:")
        success_rate = final_metrics['recall_at_5']
        if success_rate >= 0.7:
            print(f"   â€¢ Excellent retrieval performance (Recall@5 â‰¥ 70%)")
        elif success_rate >= 0.5:
            print(f"   â€¢ Good retrieval performance (Recall@5 â‰¥ 50%)")
        elif success_rate >= 0.3:
            print(f"   â€¢ Moderate retrieval performance (Recall@5 â‰¥ 30%)")
        else:
            print(f"   â€¢ Poor retrieval performance (Recall@5 < 30%)")
        
        mrr = final_metrics['mean_reciprocal_rank']
        if mrr >= 0.6:
            print(f"   â€¢ Excellent ranking quality (MRR â‰¥ 0.6)")
        elif mrr >= 0.4:
            print(f"   â€¢ Good ranking quality (MRR â‰¥ 0.4)")
        elif mrr >= 0.2:
            print(f"   â€¢ Moderate ranking quality (MRR â‰¥ 0.2)")
        else:
            print(f"   â€¢ Poor ranking quality (MRR < 0.2)")
        
        print(f"{'='*80}")


def main():
    """Main evaluation pipeline."""
    # Paths
    base_path = Path(__file__).parent.parent
    recipes_path = base_path / "data" / "processed_recipes.json"
    queries_path = base_path / "data" / "synthetic_queries.json"
    index_path = base_path / "data" / "bm25_index.pkl"
    results_path = base_path / "results" / "retrieval_evaluation.json"
    
    # Check required files
    if not recipes_path.exists():
        print(f"Processed recipes not found: {recipes_path}")
        print("Run process_recipes.py first")
        return
    
    if not queries_path.exists():
        print(f"Synthetic queries not found: {queries_path}")
        print("Run generate_queries.py first")
        return
    
    # Load data
    queries = load_queries(queries_path)
    
    if len(queries) == 0:
        print("No queries to evaluate")
        return
    
    # Create retriever
    print("Setting up BM25 retriever...")
    retriever = create_retriever(recipes_path, index_path)
    
    # Print retriever stats
    stats = retriever.get_stats()
    print(f"\nRetriever loaded: {stats['total_recipes']} recipes indexed")
    
    # Run evaluation
    evaluator = RetrievalEvaluator(retriever)
    results = evaluator.evaluate_all_queries(queries, top_k=5)
    
    # Print results
    evaluator.print_detailed_results(results, show_failures=True, max_examples=5)
    
    # Additional analysis
    evaluator.analyze_by_query_characteristics(results)
    
    # Save results
    evaluator.save_results(results, results_path, experiment_name="baseline_bm25")
    
    # Final summary
    evaluator.print_final_summary(results)


if __name__ == "__main__":
    main() 